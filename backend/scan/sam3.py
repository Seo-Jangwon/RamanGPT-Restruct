"""
SAM3 í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ê°ì²´ ìë™ ì¶”ì¶œ
"""

from ultralytics.models.sam import SAM3SemanticPredictor
import cv2
import numpy as np
import os
from pathlib import Path


def _enhance_contrast(image):
    """[ê°œì„ ] CLAHEë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ëŒ€ë¹„ í–¥ìƒ"""
    # Lab ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ê¸°(L) ì±„ë„ë§Œ ì¡°ì •
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
        
    # CLAHE ì ìš© (Clip Limitì„ ì¡°ì ˆí•˜ì—¬ ëŒ€ë¹„ ê°•ë„ ì¡°ì ˆ)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
        
    enhanced_lab = cv2.merge((cl, a, b))
    enhanced_bgr = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)
    return enhanced_bgr


def segment_with_text_prompt(image_path, text_prompts, output_dir="outputs", conf_threshold=0.25):
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*60}")
    print("SAM3 í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ì´ë¯¸ì§€: {image_path}")
    print(f"í”„ë¡¬í”„íŠ¸: {text_prompts}")
    print(f"Confidence: {conf_threshold}")
    
    # ëª¨ë¸ ì„¤ì •
    print("\n[1/5] SAM3 ëª¨ë¸ ë¡œë”©...")
    overrides = dict(
        conf=conf_threshold,
        task="segment",
        mode="predict",
        model="sam3.pt",
        half=True,
        save=False,
        verbose=False
    )
    
    predictor = SAM3SemanticPredictor(overrides=overrides)
    
    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì„¤ì •
    print("[2/5] ì´ë¯¸ì§€ ë¡œë”©...")
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    # image_enhanced = _enhance_contrast(image)
    
    h, w = image.shape[:2]
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {w} x {h}")
    
    # image = cv2.GaussianBlur(image, (5, 5), 3)

    predictor.set_image(image)
    
    # ì¶”ë¡ 
    print(f"[3/5] ì¶”ë¡  ì¤‘... (í”„ë¡¬í”„íŠ¸: {text_prompts})")
    results = predictor(text=text_prompts)
    
    # ê²°ê³¼ ì²˜ë¦¬
    print("[4/5] ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
    detected_objects = []
    obj_id = 0
    
    # ì‹œê°í™”ìš© ì´ë¯¸ì§€ ë³µì‚¬ (í•œ ë²ˆë§Œ!)
    vis_image = image.copy()
    
    for result_idx, result in enumerate(results):
        if result.masks is None:
            print(f"   í”„ë¡¬í”„íŠ¸ '{text_prompts[result_idx]}': ê°ì²´ ì—†ìŒ")
            continue
        
        num_objects = len(result.masks)
        print(f"   í”„ë¡¬í”„íŠ¸ '{text_prompts[result_idx]}': {num_objects}ê°œ ê°ì²´ ë°œê²¬")
        
        # ë§ˆìŠ¤í¬ì™€ ë°•ìŠ¤ ì¶”ì¶œ
        masks = result.masks.data.cpu().numpy()  # [N, H, W]
        boxes = result.boxes.xyxy.cpu().numpy()  # [N, 4]
        
        for i in range(num_objects):
            mask = masks[i]  # [H, W]
            
            # í”½ì…€ ì¢Œí‘œ ì¶”ì¶œ
            ys, xs = np.where(mask > 0.5)
            
            if len(xs) == 0:
                continue
            
            # ë§ˆìŠ¤í¬ì—ì„œ ì§ì ‘ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            x_min, x_max = int(xs.min()), int(xs.max())
            y_min, y_max = int(ys.min()), int(ys.max())
            bbox = [x_min, y_min, x_max, y_max]  # [x1, y1, x2, y2]
            
            # ì¤‘ì‹¬ì  ê³„ì‚°
            center_x = int((x_min + x_max) / 2)
            center_y = int((y_min + y_max) / 2)
            
            # í”½ì…€ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
            pixel_coords = [{"x": int(x), "y": int(y)} for x, y in zip(xs, ys)]
            
            # ê°ì²´ ë°ì´í„° êµ¬ì¡° (ê¸°ì¡´ SAM í˜•ì‹ê³¼ í˜¸í™˜)
            obj_data = {
                "id": obj_id,
                "center_x": center_x,
                "center_y": center_y,
                "center_type": "bbox_center",
                "pixels": pixel_coords,
                "bbox": bbox,  # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì„
                "prompt": text_prompts[result_idx] if result_idx < len(text_prompts) else "unknown"
            }
            detected_objects.append(obj_data)
            
            # ì‹œê°í™”: ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ (ì´ˆë¡ìƒ‰)
            mask_bool = mask > 0.5
            vis_image[mask_bool] = vis_image[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4
            
            # # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)
            # cv2.rectangle(vis_image, 
            #              (int(bbox[0]), int(bbox[1])), 
            #              (int(bbox[2]), int(bbox[3])), 
            #              (0, 0, 255), 2)
            
            # # ì¤‘ì‹¬ì  í‘œì‹œ (íŒŒë€ìƒ‰)
            # cv2.circle(vis_image, (center_x, center_y), 5, (255, 0, 0), -1)
            
            # ID í…ìŠ¤íŠ¸
            cv2.putText(vis_image, f"ID:{obj_id}", 
                       (center_x - 20, center_y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            
            print(f"      ê°ì²´ {obj_id}: ì¤‘ì‹¬=({center_x}, {center_y}), í”½ì…€={len(pixel_coords)}ê°œ")
            
            obj_id += 1
    
    # ê²°ê³¼ ì €ì¥
    print("[5/5] ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥
    output_image_path = os.path.join(output_dir, "sam3_result.png")
    cv2.imwrite(output_image_path, vis_image)
    print(f"   ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥: {output_image_path}")
    
    # JSON ì €ì¥
    import json
    json_path = os.path.join(output_dir, "sam3_data.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(detected_objects, f, indent=2, ensure_ascii=False)
    print(f"   JSON ë°ì´í„° ì €ì¥: {json_path}")
    
    print(f"\n{'='*60}")
    print(f"ì™„ë£Œ! ì´ {len(detected_objects)}ê°œ ê°ì²´ ê°ì§€ë¨")
    print(f"{'='*60}\n")
    
    return detected_objects


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ========== ì„¤ì • ==========
    IMAGE_PATH = "C:\\Users\\seoja\\Desktop\\RamanGPT\\RamanGPT\\backend\\tests\\data\\path3.png"
    TEXT_PROMPTS = ["cell"]  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
    OUTPUT_DIR = "./outputs"
    CONF_THRESHOLD = 0.5
    # ==========================
    
    try:
        # SAM3 ì‹¤í–‰
        detected_objects = segment_with_text_prompt(
            image_path=IMAGE_PATH,
            text_prompts=TEXT_PROMPTS,
            output_dir=OUTPUT_DIR,
            conf_threshold=CONF_THRESHOLD
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"   - ì´ ê°ì²´ ìˆ˜: {len(detected_objects)}")
        if detected_objects:
            print(f"   - ì²« ë²ˆì§¸ ê°ì²´ ì •ë³´:")
            obj = detected_objects[0]
            print(f"      ID: {obj['id']}")
            print(f"      ì¤‘ì‹¬: ({obj['center_x']}, {obj['center_y']})")
            print(f"      í”½ì…€ ìˆ˜: {len(obj['pixels'])}")
            print(f"      í”„ë¡¬í”„íŠ¸: {obj['prompt']}")
        
        return detected_objects
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
